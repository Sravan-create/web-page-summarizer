{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch webpage content and extract text\n",
    "def fetch_webpage_text(url):\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        return \"Invalid URL. Please ensure the URL starts with 'http://' or 'https://'.\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract text from <p> tags, ignoring irrelevant sections\n",
    "        text = ' '.join([p.get_text() for p in soup.find_all('p') if p.get_text()])\n",
    "        if len(text.strip()) == 0:\n",
    "            return \"No textual content found on the webpage.\"\n",
    "        return text.strip()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching the webpage: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and preprocess the extracted text\n",
    "def preprocess_text(text):\n",
    "    # Remove excessive whitespace and non-ASCII characters\n",
    "    text = ' '.join(text.split())\n",
    "    return ''.join([char for char in text if ord(char) < 128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks based on tokenization limits\n",
    "def split_text_into_tokenized_chunks(text, tokenizer, max_tokens=1024):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        tokenized_length = len(tokenizer(' '.join(current_chunk))['input_ids'])\n",
    "        if tokenized_length > max_tokens:\n",
    "            current_chunk.pop()  # Remove the last word to stay within the limit\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))  # Add the last chunk\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to truncate a summary to a specific word limit while preserving sentences\n",
    "def truncate_summary_to_word_limit(summary, word_limit=200):\n",
    "    words = summary.split()\n",
    "    if len(words) <= word_limit:\n",
    "        return summary  # Already within limit\n",
    "    truncated = ' '.join(words[:word_limit])  # Truncate to word limit\n",
    "\n",
    "    # Ensure the truncation ends at the last complete sentence\n",
    "    if '.' in truncated:\n",
    "        return truncated[:truncated.rfind('.') + 1]\n",
    "    return truncated  # Fallback to hard truncation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarize the text using a pre-trained model\n",
    "def summarize_text(text, tokenizer, max_length=200, min_length=100, final_word_limit=300):\n",
    "    try:\n",
    "        summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        chunks = split_text_into_tokenized_chunks(text, tokenizer)\n",
    "        summaries = []\n",
    "\n",
    "        print(\"\\nDebug: Tokenized chunk sizes:\")\n",
    "        for chunk in chunks:\n",
    "            print(f\"Chunk token count: {len(tokenizer(chunk)['input_ids'])}\")\n",
    "\n",
    "        for chunk in chunks:\n",
    "            summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "            summaries.append(summary[0]['summary_text'])\n",
    "\n",
    "        # Combine summaries into one and truncate to fit the final word limit\n",
    "        combined_summary = ' '.join(summaries)\n",
    "        truncated_summary = truncate_summary_to_word_limit(combined_summary, word_limit=final_word_limit)\n",
    "        return truncated_summary\n",
    "    except Exception as e:\n",
    "        return f\"Error during summarization: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching webpage content...\n",
      "\n",
      "\n",
      "Extracted text length: 11322 words\n",
      "\n",
      "Extracted text preview:\n",
      " Starbucks Corporation is an American multinational chain of coffeehouses and roastery reserves headquartered in Seattle, Washington. It was founded in 1971 by Jerry Baldwin, Zev Siegl, and Gordon Bowker at Seattle's Pike Place Market initially as a coffee bean wholesaler. Starbucks was converted into a coffee shop serving espresso-based drinks under the ownership of Howard Schultz, who was chief executive officer from 1986 to 2000 and led the aggressive expansion of the franchise across the West \n",
      "...\n",
      "\n",
      "Generating summary...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug: Tokenized chunk sizes:\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1018\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 1024\n",
      "Chunk token count: 967\n",
      "\n",
      "Summarized Text:\n",
      "\n",
      "Starbucks is an American multinational chain of coffeehouses and roastery reserves headquartered in Seattle, Washington. It was founded in 1971 by Jerry Baldwin, Zev Siegl, and Gordon Bowker at Seattle's Pike Place Market. As of November 2022, the company had 35,711 stores in 80 countries, 15,873 of which were located in the United States. Starbucks serves hot and cold drinks, whole-bean coffee, micro-ground instant coffee, espresso, caffe latte, full and loose-leaf teas, juices, Frappuccino beverages, pastries, and snacks. Depending on the country, most locations provide free Wi-Fi Internet access. In April 2003, Starbucks acquired Seattle's Best Coffee and Torrefazione Italia from AFC Enterprises for US$72 million. In September 2006, rival Diedrich Coffee announced that it would sell most of its company-owned retail stores to Starbucks. In July 2008, during the Great Recession, Starbucks announced it was closing 600 underperforming company- owned stores and cutting U.S. expansion plans. In August 2012, the largest Starbucks in the US opened at the University of Alabama's Ferguson Centre. In January 2014, as part of a change in direction, Starbucks management transitioned from a singular brand worldwide to focusing on locally relevant design for each store. On April 12, 2018, two Starbucks employees were arrested in Philadelphia. The arrests led to protests due to their apparently racially-motivated nature. CEO Kevin Johnson later apologized for the incident, and the company declined to press charges. Johnson announced that the company would close some 8000 locations on May 29 for a seminar about racial bias. In June 2019, Starbucks announced the closing of 150 locations in 2019. On March 20, 2020, due to the COVID-19 pandemic, Starbucks closed all the caf-only stores in the United States for two weeks. On October 1, 2022, Howard Schultz stepped down as CEO, with Laxman Narasimhan becoming Starbucks's next CEO.\n"
     ]
    }
   ],
   "source": [
    "# Main function to orchestrate the summarization process\n",
    "def main():\n",
    "    # Input the URL\n",
    "    url = input(\"Enter the URL of the webpage to summarize: \").strip()\n",
    "    print(\"\\nFetching webpage content...\\n\")\n",
    "    \n",
    "    # Fetch webpage content\n",
    "    webpage_text = fetch_webpage_text(url)\n",
    "    if webpage_text.startswith(\"Error\"):\n",
    "        print(webpage_text)\n",
    "        return\n",
    "\n",
    "    # Debugging: Print extracted text length\n",
    "    print(f\"\\nExtracted text length: {len(webpage_text.split())} words\\n\")\n",
    "    print(\"Extracted text preview:\\n\", webpage_text[:500], \"\\n...\")  # Show first 500 characters\n",
    "\n",
    "    # Preprocess text\n",
    "    clean_text = preprocess_text(webpage_text)\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "    # Generate summary\n",
    "    print(\"\\nGenerating summary...\\n\")\n",
    "    summary = summarize_text(clean_text, tokenizer, final_word_limit=300)\n",
    "    print(\"\\nSummarized Text:\\n\")\n",
    "    print(summary)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
